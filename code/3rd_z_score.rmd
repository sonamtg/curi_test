---
title: "Coreg_eda"
output: html_document
date: '2023-06-30'
editor_options: 
  chunk_output_type: console
---
```{r, message = FALSE}
library(tidyverse)
library(raster)
library(terra)
library(viridis)
library(dplyr)
library(RColorBrewer)
rwb <- colorRampPalette(c("red", "white", "blue"))(50)
setwd("/home/rstudio/users/gurung2/curi_test")
stor_cor_interpolation <- read_csv("data/stor_cor_interpolation.csv")
storglacier <- shapefile("data/Storglaciaren_Extents_2022/Storglaciaren_Extents_2022SEP.shp")

```


#REFER to CV_Rolstad_MedModel2.rmd for model testing and more information. 

The purpose of this rmd is to identify the z-sore, which will be used to create a variogram that will be essential for error of propagation 


## Set-UP
```{r}
#Median
## Finding the median of the elevation by elevation cuts
### Note that the centers are needed as we are making a polynomial model

stor_med_elev <- stor_cor_interpolation%>% 
  group_by(elev_cut) %>% 
  #We only used elevation because its R^2 was already within the 90s. Although the other variables were significant, they did not effect the R^2 too much
  summarise(med_elev_diff = median(elev_diff_no_outliers, na.rm = TRUE), # filled_elev_diff instead??
         count = n()) %>% #counts will play a role in making the model
  ungroup() %>% 
     mutate(lower_elev = as.numeric(gsub("\\((.*),.*\\]", "\\1", elev_cut)),
            upper_elev = as.numeric(gsub(".*,(.*)\\]", "\\1", elev_cut)),
          med_elev_cut = (lower_elev + upper_elev) / 2,
          cen_elev_med = med_elev_cut - median(med_elev_cut)) #1450 instead of 1475 like the one in z_scoreLBV 

sapply(lapply(stor_med_elev, unique), length) #helps identify the different unique levels/values in the data of each col

  

#NMAD
#data frame that is going to be used for modeling 
### Unsure, come back to
stor_slope_maxcurv <- stor_cor_interpolation %>%
  filter(is.glacier == 1)%>%
  mutate(max_curv_5 = ifelse(max_curv < 5, max_curv, NA)) %>% 
  # combined the two largest max curve bins and the three largest slope bins to keep the count consistent
 #mutate(max_curve_bin = fct_collapse(max_curve_bin, "(2.45,338]" = c("(2.45, 3.46]", "(3.46,338]"))) %>% 
  mutate(slope_cut = cut (slope_degrees,breaks = c(0,5,10,15,20,Inf) ))%>%
  mutate(elev_cut = cut (elev,breaks = c(1150,1250,1300,1350,1400,1450,1500,1550,1600,1750,Inf) ))%>%
  #mutate(slope_cut = fct_collapse(slope_cut, "(16,80.5]" = c("(16,20.1]","(20.1,28.9]","(28.9,80.5]"))) %>% 
  mutate(slope_cut = fct_reorder(slope_cut, slope_degrees))%>%
  mutate(elev_cut = fct_reorder(elev_cut, elev))%>%
  group_by(slope_cut, elev_cut) %>% 
  summarise(med_elev_diff = median(elev_diff_no_outliers, na.rm = TRUE), 
         mad_elev_diff = mad(elev_diff_no_outliers, na.rm = TRUE),
         med_slope = median(slope_degrees),
         med_elev = median(elev, na.rm = TRUE),
         ct = n()) %>% 
  ungroup() %>%
  mutate(cen_elev = med_elev - median(med_elev, na.rm = TRUE)) %>% #center of max curvature 
  mutate(cen_slope = med_slope - median(med_slope, na.rm = TRUE))%>%#center of slope 
mutate(lower_elev = as.numeric(gsub("\\((.*),.*\\]", "\\1", elev_cut)),
            upper_elev = as.numeric(gsub(".*,(.*)\\]", "\\1", elev_cut)),
          med_elev_cut = (lower_elev + upper_elev) / 2,
          cen_elev_med = med_elev_cut - median(med_elev_cut))

# stor_slope_maxcurv %>% 
#   group_by(max_curve_bin) %>% 
#   summarise(n = n()) %>% 
#   print(n = Inf) 





```

#Dataset to predict on
```{r}
#data frame used to predict nmad and median
## For the data fram we will predict on, we used their actual variables instead of cuts to help center it.

stor_cor_interpolation <- stor_cor_interpolation %>%
  mutate(#cen_elev_med = elev - median(stor_med_elev$med_elev_cut),  #1450
        cen_slope = slope_degrees - median(stor_slope_maxcurv$med_slope)) #1.026125)
```


#Med_Model & predict
```{r}
med_model <- lm(med_elev_diff ~ polym(cen_elev_med, degree=3, raw=TRUE), data = stor_med_elev, weights = count) #we choose this model despite having significance in the other ones b/c elev already accounted for 90% of the data (R^2 =90+%)
summary(med_model)
#stor_med_elev$med_residuals <- c(NA, residuals(med_model)) #???


#predicting for Med for every point
stor_cor_interpolation$pred_med_elev_diff <- predict(med_model, newdata = stor_cor_interpolation)
```

#NMAD_Model & predict
```{r}
#models 

mad_model_1 <- lm(mad_elev_diff ~ polym(cen_elev_med,cen_slope,degree=3, raw=TRUE), data = stor_slope_maxcurv)

stor_cor_interpolation$pred_mad <- predict(mad_model_1, newdata = stor_cor_interpolation)



``` 


## There are negative predicted MAD in stor_no_out
```{r}
stor_cor_interpolation$pred_mad %>% summary()

stor_cor_interpolation %>%
  slice_sample(n = 5000) %>%
  ggplot(aes(cen_slope, pred_mad, col = max_curv)) + 
  geom_point()

```



#Z-score
```{r}

z_score <- stor_cor_interpolation %>% 
  mutate(z_score = (elev_diff_no_outliers - pred_med_elev_diff) / pred_mad)



summary(z_score$z_score)
sd(z_score$z_score, na.rm = TRUE)
hist(z_score$z_score)
hist(z_score$pred_mad)

```
 
 
```{r}

#pdf("~z_score histogram.pdf", width = 6.5, height = 4)

hist(stor_cor_df$z_score)

#dev.off()

#pdf("~predicted mad histogram.pdf", width = 6.5, height = 4)

hist(stor_cor_df$pred_mad)

#dev.off()

```


```{r}
#plots used on paper and graph from this rmd


#pdf("~z_score histogram.pdf", width = 6.5, height = 4)

stor_cor_interpolation %>%
  slice_sample(n = 5000) %>%
  ggplot(aes(cen_slope, pred_mad, col = max_curv)) + 
  geom_point()
#dev.off()



#data set used to for the next three plots

stor_slope_maxcurv_cen <- stor_slope_maxcurv %>% #same thing but uses center slope and switched our condition to less than 1 
  mutate(cen_max = ifelse(cen_max > 2.45, 3.46 - median(stor_slope_maxcurv$med_max), cen_max)) %>%  #double check this with the max bins
  #mutate(med_slope = ifelse(med_slope > 30, 30, med_slope), 
  mutate(pred_mad = ifelse(cen_slope <11.1 - median(stor_slope_maxcurv$med_slope), predict(model1_cen, newdata = .), predict(model2_cen, newdata = .))) 

#pdf("~center max vs mad elevation diff.pdf", width = 6.5, height = 4)

stor_slope_maxcurv_cen %>% 
  ggplot(aes(cen_max, mad_elev_diff)) +
  geom_point()

#dev.off()


#pdf("~center slope and max curve vs mad elev diff and predicted elev diff .pdf", width = 6.5, height = 4)
stor_slope_maxcurv_cen %>% 
  ggplot(aes(x = cen_slope, color=max_curve_bin)) +
  geom_point(aes(y = mad_elev_diff)) +
  geom_line(aes(y = pred_mad)) 
#dev.off()


stor_cor_df %>% 
  slice_sample(n = 5000)%>%
  mutate(mad_elev_diff = mad(elev_diff)) %>% 
  ggplot(aes(x = cen_slope, color=max_curve_bin)) +
  geom_point(aes(y = mad_elev_diff)) +
  geom_line(aes(y = pred_mad))




# huggonet slope and max curvature plot 

#pdf("~max curvature and slope bin heat map.pdf", width = 6.5, height = 4)
 ggplot(data
        = stor_slope_maxcurv, aes(x = fct_reorder(slope_cut, med_slope), y = fct_reorder(max_curve_bin, cen_max), fill = mad_elev_diff)) +
  geom_tile() +
  scale_fill_viridis_c() +
  labs(title = "Heatmap of slope and maximum curvature",
       x = "Slope bin",
       y = "Max curvature bin",
       fill = "NMAD Elevation Difference") +
  theme_minimal() +
  theme(
    axis.text.x = element_text(angle = 45, hjust = 1), 
    panel.grid.major = element_blank(),  
    panel.grid.minor = element_blank(),  
    legend.position = "right"  # Move legend to the right side of the plot
  )
 #dev.off()
  
 
 
 # convert to raster
 stor_z_raster <- rasterFromXYZ(stor_cor_df)
 
names(stor_z_raster) #our vairbales 

pred_med <- stor_z_raster[[ "pred_med_elev_diff" ]] #selected detrending 
pred_mad <- stor_z_raster[["pred_mad"]] # selecting standardization 
z_score <- stor_z_raster[["z_score"]] # selecting standardization 
 
plot(pred_med)
plot(pred_mad)


#pdf("~standardization.pdf", width = 6.5, height = 4)

subset_extent3 <- c(650500,650700,7536500 , 7536680)
par(mfrow= c(1,2))
plot(z_score, col = rwb, zlim = c(-15,15)) 
plot(storglacier, add = TRUE)
plot(extent(subset_extent3), add = TRUE)
ddemzoom_cropped <- crop(z_score, extent(subset_extent3))%>%
  trim()
plot(ddemzoom_cropped, main = "
     Standardized elevation diffrence", col = rwb, zlim = c(-15,15))
#dev.off()
```




```{r}
# stor_cor_df %>% 
#    slice_sample(n=5000) %>% 
#   ggplot(aes(x = cen_slope, y = pred_mad)) +
#   geom_point()
# 
# stor_cor_df %>% 
#   slice_sample(n=5000) %>% 
#   ggplot(aes(x = max_curv, y = pred_mad)) +
#   geom_point()
# # cen_slope > 30, do the same thing like the 
# 
# stor_cor_df %>% 
#   slice_sample(n=5000) %>% 
#   ggplot(aes(x = pred_mad, y = slope_degrees, color = max_curv)) +
#   geom_point()
# 
# stor_cor_df %>%
#   slice_sample(n = 5000) %>%
#   ggplot(aes(cen_slope, pred_mad, col = max_curv)) +
#   geom_point()
# 
# stor_cor %>%
#   filter(pred_mad < 0) %>%
#   ggplot(aes(cen_slope, cen_max, col = pred_mad)) +
#   geom_point()
#   

```

After this rmd, please check out Variogram-Rolstad (variogram that is spherical), from the rolstad reading. For the Variogram that follows Hugonnet, then check out nmad_variigram.rmd
 
```{r}
stor_cor_zscore <- z_score %>%
  dplyr::select("z_score","x","y")
#write_csv(stor_cor_zscore, "data/stor_cor_zscore.csv") 

```

